# -*- coding: utf-8 -*-
"""Cointegration Proof of Concept.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nCr9B9jSu2ud0GVITUvvQwoBN2TldSKp
"""

import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.tsa.stattools import coint

# Paths to CSVs
FILE_1 = "/content/polymarket-price-data-24-04-2024-13-11-2024-1764802558741.csv"
FILE_2 = "/content/polymarket-price-data-09-01-2024-11-11-2024-1764802540569.csv"

# Column names in those CSVs
# Guessing common names – change these if needed - change price col to outcome to compare
TIME_COL_1 = "Date (UTC)"   # e.g. "timestamp", "time", "t"
TIME_COL_2 = "Date (UTC)"

PRICE_COL_1 = "Republicans sweep"      # e.g. "price", "mid_price", "p"
PRICE_COL_2 = "Donald Trump"

def load_price_series(filepath: str, time_col: str, price_col: str) -> pd.Series:
    """
    Load a CSV and return a pandas Series of prices indexed by datetime.
    """
    df = pd.read_csv(filepath)

    # Parse timestamp → datetime index
    df[time_col] = pd.to_datetime(df[time_col], utc=True, errors="coerce")
    df = df.dropna(subset=[time_col, price_col])

    df = df.sort_values(time_col)
    df = df.set_index(time_col)

    # Ensure float prices
    s = pd.to_numeric(df[price_col], errors="coerce").dropna()
    s.name = price_col
    return s

s1 = load_price_series(FILE_1, TIME_COL_1, PRICE_COL_1)
s2 = load_price_series(FILE_2, TIME_COL_2, PRICE_COL_2)

print("Loaded series lengths:", len(s1), len(s2))

Z_THRESHOLD = 2.0  # |z| > 2 ~ big deviation

df_pair = pd.concat(
    [s1.rename("m1"), s2.rename("m2")],
    axis=1
).dropna()

if df_pair.empty:
    raise ValueError("After aligning timestamps, there are no overlapping data points. "
                     "Check that the time ranges and time formats match.")

print("Aligned series length:", len(df_pair))

# COINTEGRATION (ENGLE–GRANGER)


# Log prices are typically used for cointegration tests
y0 = np.log(df_pair["m1"])
y1 = np.log(df_pair["m2"])

score, pvalue, crit_vals = coint(y0, y1)

print("\n=== Engle–Granger Cointegration Test ===")
print(f"Test statistic: {score}")
print(f"P-value:       {pvalue}")
print(f"Critical values (1%, 5%, 10%): {crit_vals}")

# Estimate cointegrating vector: y0 = a + beta*y1 + e
X = sm.add_constant(y1)
res = sm.OLS(y0, X).fit()
beta = res.params[1]

print("\nEstimated cointegrating vector: (1, -beta)")
print(f"beta = {beta}")

# BUILD SPREAD & Z-SCORE


spread = y0 - beta * y1
spread.name = "spread"

spread_z = (spread - spread.mean()) / spread.std()
spread_z.name = "spread_z"

df_spread = pd.concat([df_pair, spread, spread_z], axis=1)

# FLAG ARBITRAGE WINDOWS


signals = df_spread[spread_z.abs() > Z_THRESHOLD].copy()
signals["side"] = np.where(
    signals["spread_z"] > 0,
    "short m1 / long m2",   # m1 rich vs m2
    "long m1 / short m2"    # m1 cheap vs m2
)

print(f"\n=== Arbitrage Signal Summary (|z| > {Z_THRESHOLD}) ===")
print(f"Total candidate points: {len(signals)}")

if not signals.empty:
    print("\nSample signals:")
    print(signals[["m1", "m2", "spread_z", "side"]].head(10))

    # Optionally save to CSV t
    signals.to_csv("cointegration_arbitrage_signals.csv")
    print("\nSaved detailed signals to 'cointegration_arbitrage_signals.csv'")
else:
    print("No large spread deviations detected under this threshold.")